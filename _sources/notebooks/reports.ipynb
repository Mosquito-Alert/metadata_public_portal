{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competitive-abortion",
   "metadata": {},
   "source": [
    "# Dataset: _reports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests, zipfile, io, json, re, os, glob, shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "import src.utils as ut\n",
    "\n",
    "# Setup the root path of the application\n",
    "project_path = ut.project_path()\n",
    "\n",
    "# Load the metadata\n",
    "\n",
    "meta_filename = [\n",
    "    f\"{ut.project_path(1)}/meta/mosquito_alert/reports.json\",\n",
    "    f\"{ut.project_path(2)}/meta_ipynb/reports.html\",\n",
    "]\n",
    "metadata = ut.load_metadata(meta_filename)\n",
    "\n",
    "# Get contentUrl from metadata file\n",
    "ut.info_meta(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-cambridge",
   "metadata": {},
   "source": [
    "## 1. Distribution from Zenodo cloud\n",
    "\n",
    "This dataset is updated nightly and the most recent version can be downloaded\n",
    "from Zenodo at https://doi.org/10.5281/zenodo.597466. This URL will always\n",
    "resolve to the most recent version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=0, idx_hasPart=None\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and open the zip container\n",
    "\n",
    "# Get the latest zenodo file version of the dataset\n",
    "r = requests.get(contentUrl)\n",
    "file_url = BeautifulSoup(r.content, \"html.parser\").find(\"a\", {\"class\": \"filename\"})[\n",
    "    \"href\"\n",
    "]\n",
    "file_contentUrl = urllib.parse.urljoin(r.url, file_url)\n",
    "\n",
    "# Download the dataset\n",
    "r_file = requests.get(file_contentUrl)\n",
    "z = zipfile.ZipFile(io.BytesIO(r_file.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-stand",
   "metadata": {},
   "source": [
    "We have the option to extract all the file reports into a distribution folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract everything that was downloaded\n",
    "z.extractall(path)\n",
    "\n",
    "# Extract all the reports\n",
    "for f in glob.glob(f\"{path}/*/*.zip\"):\n",
    "    with zipfile.ZipFile(f, \"r\") as zipObj:\n",
    "        for zip in zipObj.filelist:\n",
    "            filename = os.path.basename(zip.filename)\n",
    "            with open(f\"{path}/{filename}\", \"wb\") as f:\n",
    "                f.write(zipObj.open(zip).read())\n",
    "\n",
    "# Move the translation_dict and remove the download folder\n",
    "trans_dict = \"translation_dict.json\"\n",
    "path_download = f\"{path}/{z.filelist[0].filename}\"\n",
    "shutil.move(path_download + trans_dict, path + \"/\" + trans_dict)\n",
    "shutil.rmtree(path_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-bermuda",
   "metadata": {},
   "source": [
    "Or we could concatenate all reports into a single dataframe before and save\n",
    "it as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all reports into a dataframe. Json is a nested data format, thus one should\n",
    "# decide to what level data should be unrolled (i.e. json sublevels are transformed\n",
    "# to new columns). Note that json flattening implies a bigger table with many columns.\n",
    "# Here we provide two options: a compact table (level = 0) and fully expanded\n",
    "# table (level = None). For more information, see Pandas function\n",
    "# [_json_normalize_](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html)\n",
    "\n",
    "level = 0  # normalize only the root level (keys->column names, values->table values)\n",
    "# level = None  # normalize all levels\n",
    "\n",
    "df_reports = []\n",
    "df_responses = []\n",
    "\n",
    "for report_name in glob.glob(f\"{path}/all_reports*.json\"):\n",
    "    with open(report_name) as f:\n",
    "        d = json.loads(f.read())\n",
    "    df_dict = pd.json_normalize(d, max_level=level)\n",
    "    df_reports.append(df_dict)\n",
    "    if level is None:\n",
    "        df_list = pd.json_normalize(d, max_level=level, record_path=[\"responses\"])\n",
    "        df_responses.append(df_list)\n",
    "\n",
    "df = pd.concat(df_reports)\n",
    "if level is None:\n",
    "    df_responses = pd.concat(df_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-whale",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If we normalize all the levels (i.e. level = None), than the _responses_ attribute\n",
    "of reports dataset is a list of dictionaries and thus it is bypassed by\n",
    "_json_normalize_. In this case, we need to explicitly flatten _responses_\n",
    "and concatenate with all the other attributes. On the other hand, if the table\n",
    "is compact (i.e. level = 0), we could use _json_normalize_ to unroll the attributes\n",
    "of our interest but first we need to transform them in a dictionary data format.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859a36e",
   "metadata": {},
   "source": [
    "Let's make an example on how to unroll nested attributes. Suppose that would\n",
    "like to study mosquito bites in Netherlands. This information is given by the\n",
    "attribute _responses_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll the _responses_ attribute if we only unrolled the root level\n",
    "if level == 0:\n",
    "    col = \"responses\"\n",
    "    df_responses = pd.json_normalize(\n",
    "        df[[col]].to_dict(orient=\"records\"), record_path=col\n",
    "    )\n",
    "\n",
    "# Join responses with some usefull attributes from the main table to filter and\n",
    "# group bites over space and time\n",
    "col = [\"version_UUID\", \"creation_year\", \"creation_month\", \"type\", \"country\"]\n",
    "df_responses = df_responses.merge(\n",
    "    df[col], left_on=\"report\", right_on=\"version_UUID\", validate=\"m:m\"\n",
    ").drop([\"report\"], axis=1)\n",
    "\n",
    "# Transform float types to integers. Note that Int' has a NaN type, while 'int'\n",
    "# do not have NaNs thus filtering is not possible if missing values are present.\n",
    "df_responses = df_responses.astype(\n",
    "    {\n",
    "        \"question_id\": \"Int16\",\n",
    "        \"answer_id\": \"Int16\",\n",
    "        \"answer_value\": \"Int16\",\n",
    "    }\n",
    ")\n",
    "# Filter for total mosquito bites registered by a user in Netherlands (code ISO3166-alpha3)\n",
    "df_bite = df_responses.query(\"type == 'bite' & country == 'NLD' & question_id == 1\")\n",
    "\n",
    "# Count bites on monthly basis grouped by year\n",
    "df_bite_stats = df_bite.groupby([\"creation_year\", \"creation_month\"])[\n",
    "    \"answer_value\"\n",
    "].sum()\n",
    "\n",
    "#  Plot the temporal coverage of bites for Netherlands on log-scale\n",
    "p = df_bite_stats.unstack(level=0).plot(\n",
    "    kind=\"bar\", stacked=True, logy=True, ylabel=\"Bite counts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df74e7",
   "metadata": {},
   "source": [
    "Some attributes of reports are key-value json-like data, that need additional\n",
    "tables to be fully comprehensive (for example, tiger_responses, responses, etc).\n",
    "Since multilanguage translations are available, we make language as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open translation dictionary\n",
    "with open(path + \"/\" + trans_dict) as f:\n",
    "    r = f.read()\n",
    "\n",
    "try:\n",
    "    d = json.loads(r)\n",
    "except ValueError:\n",
    "    print(\"Warning: not a valid Json format. Try to get rid of trailing comma.\")\n",
    "try:\n",
    "    r = re.sub(r\"\\\"\\s*,\\s*\\}\", '\" }', r)\n",
    "    d = json.loads(r)\n",
    "except ValueError:\n",
    "    print(\"Json format is still not valid.\")\n",
    "\n",
    "df_reports_translation = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "df_reports_translation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d37b75",
   "metadata": {},
   "source": [
    "Imagine that you need to translate the questions and answers of mosquito bites\n",
    "(i.e. _df_responses_ dataframe) in another language. The above translation table\n",
    "could do this job for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, let's translate to English.\n",
    "lang = \"en\"\n",
    "translation = df_reports_translation.loc[lang].to_dict()\n",
    "answers = df_responses[\"answer\"].map(translation)\n",
    "questions = df_responses[\"question\"].map(translation)\n",
    "df_responses.insert(1, f\"translated_answer_{lang}\", answers)\n",
    "df_responses.insert(1, f\"translated_question_{lang}\", questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a06d92",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Not all the reports could be translated since they miss the question and\n",
    "answer ID tag due to an older version of the Mosquito Alert mobile app. Starting\n",
    "from 2020, translations are available.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reports on CSV or parquet\n",
    "filename = f\"{path}/all_reports\"\n",
    "df.to_parquet(f\"{filename}.parquet\")  # very low file-size (need to install pyArrow)\n",
    "df.to_csv(f\"{filename}.csv\")  # x10 size if compared with the dataframe\n",
    "\n",
    "# Save the reports responses unrolled\n",
    "df_responses.to_parquet(f\"{filename}_responses.parquet\")\n",
    "df_responses.to_csv(f\"{filename}_responses.csv\")\n",
    "\n",
    "# Save seports translation on CSV\n",
    "df_reports_translation.to_csv(f\"{filename}_translation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-innocent",
   "metadata": {},
   "source": [
    "## 2. Distribution from MosquitoAlert Github repository\n",
    "\n",
    "This dataset is also updated daily on GitHub and can be accessed from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=1, idx_hasPart=None\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "r_file = requests.get(contentUrl[0])\n",
    "z = zipfile.ZipFile(io.BytesIO(r_file.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca5c09",
   "metadata": {},
   "source": [
    "We have the option to extract all the file reports into a distribution folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract everything that was downloaded\n",
    "z.extractall(path)\n",
    "\n",
    "# Move the report files and remove the download folder\n",
    "for file in glob.glob(f\"{path}/**/*.json\", recursive=True):\n",
    "    shutil.copy(file, path)\n",
    "\n",
    "path_download = os.path.dirname(file)\n",
    "shutil.rmtree(path_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea10aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The procedure of conversion from json to csv is equal to the Zenodo distribution example\n",
    "\n",
    "level = 0  # normalize only the root level (keys->column names, values->table values)\n",
    "# level = None  # normalize all levels\n",
    "\n",
    "df_reports = []\n",
    "df_responses = []\n",
    "\n",
    "for report_name in glob.glob(f\"{path}/all_reports*.json\"):\n",
    "    with open(report_name) as f:\n",
    "        d = json.loads(f.read())\n",
    "    df_dict = pd.json_normalize(d, max_level=level)\n",
    "    df_reports.append(df_dict)\n",
    "    if level is None:\n",
    "        df_list = pd.json_normalize(d, max_level=level, record_path=[\"responses\"])\n",
    "        df_responses.append(df_list)\n",
    "\n",
    "df = pd.concat(df_reports)\n",
    "if level is None:\n",
    "    df_responses = pd.concat(df_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e14e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request other support material of the reports and put them into dataframes\n",
    "# Since multilanguage translations are available, we make language as index\n",
    "\n",
    "url = contentUrl[-1]\n",
    "trans_dict = f\"{path}/translation_dict.json\"\n",
    "ut.download_file(url, trans_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addcf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open translation dictionary\n",
    "with open(trans_dict) as f:\n",
    "    r = f.read()\n",
    "\n",
    "try:\n",
    "    d = json.loads(r)\n",
    "except ValueError:\n",
    "    print(\"Warning: not a valid Json format. Try to get rid of trailing comma.\")\n",
    "try:\n",
    "    r = re.sub(r\"\\\"\\s*,\\s*\\}\", '\" }', r)\n",
    "    d = json.loads(r)\n",
    "except ValueError:\n",
    "    print(\"Json format is still not valid.\")\n",
    "\n",
    "df_reports_translation = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "df_reports_translation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reports on CSV or parquet\n",
    "filename = f\"{path}/all_reports\"\n",
    "df.to_parquet(f\"{filename}.parquet\")  # very low file-size (need to install pyArrow)\n",
    "df.to_csv(f\"{filename}.csv\")  # x10 size if compared with the dataframe\n",
    "\n",
    "# Save seports translation on CSV\n",
    "df_reports_translation.to_csv(f\"{filename}_translation.csv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
