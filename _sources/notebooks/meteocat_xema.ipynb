{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pregnant-gnome",
   "metadata": {},
   "source": [
    "# DataCatalog: _meteocat_xema_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-hardwood",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import src.utils as ut\n",
    "\n",
    "# Setup the root path of the application\n",
    "project_path = ut.project_path()\n",
    "\n",
    "# Load the metadata\n",
    "\n",
    "meta_filename = [\n",
    "    f\"{ut.project_path(1)}/meta/environment/meteocat_xema.json\",\n",
    "    f\"{ut.project_path(2)}/meta_ipynb/meteocat_xema.html\",\n",
    "]\n",
    "metadata = ut.load_metadata(meta_filename)\n",
    "\n",
    "# Get contentUrl from metadata file\n",
    "ut.info_meta(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-probability",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 1. Distribution of the entire dataset within CSV download\n",
    "\n",
    "First we need to download the hole dataset as a CSV-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset meteocat_xema_data form the meteocat_xema catalog\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=0, idx_hasPart=0\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset file (big files could get a while to download)\n",
    "filename = f\"{path}/dataset.csv\"\n",
    "ut.download_file(contentUrl, filename, method=\"curl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-realtor",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.1. Copy from CSV to PostgreSQL database\n",
    "\n",
    "Here we assume that PostgreSQL database server is available on the user's\n",
    "machine. From contentUrl we download the dataset. For convenience, let's rename\n",
    "it as dataset.csv). We use the following SQL code to generate a\n",
    "new table and copy the CSV-dataset content. Note that the CSV-file could be\n",
    "badly formatted in some row and that the copy operation will exit with error.\n",
    "Thus it is important to check and correct (drop the inconsistent rows) of the\n",
    "CSV-file if needed.\n",
    "\n",
    "```sql\n",
    "  DROP TABLE IF EXISTS xema;\n",
    "\n",
    "  CREATE TABLE IF NOT EXISTS xema (\n",
    "\t    ID text,\n",
    "\t    CODI_ESTACIO text,\n",
    "\t    CODI_VARIABLE text,\n",
    "\t    DATA_LECTURA timestamp,\n",
    "\t    DATA_EXTREM timestamp,\n",
    "\t    VALOR_LECTURA FLOAT8,\n",
    "\t    CODI_ESTAT text,\n",
    "\t    CODI_BASE text\n",
    "  );\n",
    "\n",
    "  COPY xema  FROM './[path]/dataset.csv' DELIMITER ',' HEADER CSV;\n",
    "```\n",
    "\n",
    "At this point we imported into PostgreSQL the dataset table, but the data\n",
    "types of the attributes could be redefined to save some memory and accelerate\n",
    "filtering operations.\n",
    "\n",
    "```sql\n",
    "   ALTER TABLE xema\n",
    "      ALTER COLUMN id TYPE VARCHAR (14),\n",
    "      ALTER COLUMN codi_estacio TYPE VARCHAR (2),\n",
    "      ALTER COLUMN codi_variable TYPE INT USING codi_variable::integer,\n",
    "      ALTER COLUMN valor_lectura TYPE FLOAT4,\n",
    "      ALTER COLUMN codi_estat TYPE VARCHAR (1),\n",
    "      ALTER COLUMN codi_base TYPE VARCHAR (2);\n",
    "```\n",
    "\n",
    "Finally, assume that we are interested in the weather station with code 'KP'.\n",
    "Below, we select the station and export it to a CSV file.\n",
    "\n",
    "```sql\n",
    "   COPY (\n",
    "      SELECT codi_variable, data_lectura, data_extrem, valor_lectura, codi_estat\n",
    "      FROM kp_station ks)\n",
    "      TO '/path/kp_station.csv' DELIMITER ',' CSV HEADER;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-haven",
   "metadata": {},
   "source": [
    "## 1.2. Distributed computation read of big-CSV files\n",
    "\n",
    "Another alternative to query directly the downloaded CSV dataset is to use\n",
    "Dask Python library. Dask allows us to use parallel computing by chunking the\n",
    "file-read. Here we use Dask on a single machine (our personal computer), but\n",
    "it could be easily setup to work on distributed cluster machines.\n",
    "\n",
    "First we start a Dask client and we setup it to work with 4 threads with a\n",
    "memory limit of 2GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "client = Client(n_workers=1, threads_per_worker=4, processes=False, memory_limit=\"1GB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-buddy",
   "metadata": {},
   "source": [
    "At this point, use the Dask dataframe in the same fashion as we would use Pandas.\n",
    "Note that the read_csv operation has not executed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    \"CODI_ESTACIO\": \"category\",\n",
    "    \"CODI_VARIABLE\": \"float\",\n",
    "    \"VALOR_LECTURA\": \"float\",\n",
    "    \"CODI_ESTAT\": \"category\",\n",
    "    \"CODI_BASE\": \"category\",\n",
    "}\n",
    "\n",
    "df = dd.read_csv(filename, parse_dates=[\"DATA_LECTURA\", \"DATA_EXTREM\"], dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-blair",
   "metadata": {},
   "source": [
    "Since Dask works with lazy computation, we should specify when we actually\n",
    "would like to compute the chain of operations that we've setup. This is\n",
    "because Dask first needs to build a directed acyclic diagram (DAG) of workers.\n",
    "For example, to query two weather stations from the dataset and we run the\n",
    "Dask client with the .compute() operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.query(\"(CODI_ESTACIO == 'UR' or CODI_ESTACIO == 'KP')\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-richmond",
   "metadata": {},
   "source": [
    "Note that with the above Dask client settings (computation resources) the\n",
    "query operation runs for a couple of hours. On the other hand, the SQL\n",
    "approach is much faster even if it runs on a single core because of its\n",
    "internal optimizations (compiled code and key-index). The good point of Dask\n",
    "is that it is easy to scale-up if more computation power is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-packaging",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Distribution from Socrata API\n",
    "\n",
    "This example shows how to get weather station data from the Socrate API and store\n",
    "them into a pandas dataframe. Here no storage capacity or setup of a SQL\n",
    "server is needed since the database and the query operations are managed by\n",
    "the API' web-server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-niagara",
   "metadata": {},
   "source": [
    "First we will get a list of available stations of the XEMA network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset meteocat_xema_stations form the api end-point distribution\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=1, idx_hasPart=1\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-liquid",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note that since we use Socrata API library to request data we need to provide\n",
    "a the source domain and a dataset identifier. Note that the API will warn us\n",
    "if we don't provide an app-token but for this dataset there is actually no\n",
    "limitation to data access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the domain and dataset identifier from metadata\n",
    "url_split = contentUrl.split(\"/\")\n",
    "domain = url_split[2]\n",
    "dataset_id = url_split[-1].split(\".\")[0]\n",
    "print(f'Domain is \"{domain}\" with dataset identifier \"{dataset_id}\".')\n",
    "\n",
    "# Setup a client to a Socrata domain\n",
    "client = Socrata(domain, None, timeout=10)\n",
    "# Setup the end-point to\n",
    "stations = client.get(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-rebel",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can display the list of all the available weather station as a dataframe table\n",
    "and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = pd.DataFrame.from_records(stations, coerce_float=True)\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on CSV-file\n",
    "filename = f\"{path}/dataset.csv\"\n",
    "df_stations.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-spyware",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Each station has a variety of measurement devices which are listed in\n",
    "meteocat_xema_variables dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset meteocat_xema_variables and the api end-point distribution\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=1, idx_hasPart=2\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset identifier from metadata\n",
    "url_split = contentUrl.split(\"/\")\n",
    "dataset_id = url_split[-1].split(\".\")[0]\n",
    "print(f'Domain is \"{domain}\" with dataset identifier \"{dataset_id}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data that describes measurement devices\n",
    "variables = client.get(dataset_id)\n",
    "df_variables = pd.DataFrame.from_records(variables, coerce_float=True)\n",
    "df_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on CSV-file\n",
    "filename = f\"{path}/dataset.csv\"\n",
    "df_stations.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-saying",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "At this point, assume that we are interested to get data for the weather\n",
    "station with code _KP_ relative to _Fogars de la Selva_. For this we need\n",
    "to provide the dataset identifier of _meteocat_xema_data_ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.query(\"codi_estacio == 'KP'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset meteocat_xema_data form the api end-point distribution\n",
    "contentUrl, dataset_name, distr_name = ut.get_meta(\n",
    "    metadata, idx_distribution=1, idx_hasPart=0\n",
    ")\n",
    "\n",
    "# Make folders for data download\n",
    "path = f\"{project_path}/data/{dataset_name}/{distr_name}\"\n",
    "ut.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset identifier from metadata\n",
    "url_split = contentUrl.split(\"/\")\n",
    "dataset_id = url_split[-1].split(\".\")[0]\n",
    "print(f'Domain is \"{domain}\" with dataset identifier \"{dataset_id}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-gibraltar",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "It is a good idea to check the metadata of the dataset. In our case we check\n",
    "the total number of records in the meteocat_xema_data dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata and max number of records available\n",
    "meta = client.get_metadata(dataset_id)\n",
    "cachedContents = meta[\"columns\"][0][\"cachedContents\"]\n",
    "limit = int(cachedContents[\"non_null\"]) + int(cachedContents[\"null\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-polyester",
   "metadata": {},
   "source": [
    "Once we choose one or more weather stations to query, we are ready to get a\n",
    "JSON data-response from the API. We specify how many last records\n",
    "we need by the parameter _limit_. If we do not assign any value to limit, it\n",
    "will get at most the first 1000 records.\n",
    "\n",
    "If we would like to get a set of stations it is more convenient to\n",
    "setup a threaded api call. It is possible to perform a query with a sql\n",
    "statement like `IN ('KP', 'J5', ...)` but the output dataframe could exceed\n",
    "the available memory available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getData(dataset_id, code, min_date=\"\", limit=1000, path=None):\n",
    "\n",
    "    # Query the data by code station and datetime if given\n",
    "    if min_date != \"\":\n",
    "        where = f\"codi_estacio = '{code}' AND data_lectura > '{min_date}'\"\n",
    "    else:\n",
    "        where = f\"codi_estacio = '{code}'\"\n",
    "\n",
    "    try:\n",
    "        records = client.get(dataset_id, where=where, limit=limit)\n",
    "    except:\n",
    "        if min_date != \"\":\n",
    "            print(f\"\\n Station {code}: retry without datetime filter\")\n",
    "            where = f\"codi_estacio = '{code}'\"\n",
    "            records = client.get(dataset_id, where=where, limit=limit)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records, coerce_float=True)\n",
    "\n",
    "    # Convert to proper datatype\n",
    "    column_types = {\n",
    "        \"data_lectura\": \"datetime64\",\n",
    "        \"data_extrem\": \"datetime64\",\n",
    "        \"codi_estacio\": \"category\",\n",
    "        \"codi_estat\": \"category\",\n",
    "        \"codi_base\": \"category\",\n",
    "        \"codi_variable\": \"int\",\n",
    "        \"valor_lectura\": \"float\",\n",
    "    }\n",
    "    df = df.astype(column_types)\n",
    "\n",
    "    # Apply datetime filter\n",
    "    if min_date != \"\":\n",
    "        df = df.query(f\"data_lectura > '{min_date}'\")\n",
    "\n",
    "    if path is not None:\n",
    "        # Save on CSV-file and parquet (note the storage saving with parquet)\n",
    "        filename = f\"{path}/dataset_{code}\"\n",
    "        df.to_csv(filename + \".csv\")\n",
    "        df.to_parquet(filename + \".parquet\")\n",
    "        print(f\"Saved on {filename}\")\n",
    "        gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For only one station we simply run getData\n",
    "min_date = \"2020-01-01\"\n",
    "codes = [\"XV\"]\n",
    "df = getData(dataset_id, code=codes[0], min_date=min_date, limit=limit, path=path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But for a set of stations we can run a threaded api call\n",
    "min_date = \"2020-01-01\"\n",
    "codes = [\"KP\", \"J5\", \"UC\", \"XZ\", \"DO\", \"UB\", \"U2\", \"DJ\", \"WT\", \"D4\", \"DF\", \"W1\", \"XJ\"]\n",
    "\n",
    "# Put few workers in order to prevent memory overflow\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for code in codes:\n",
    "        executor.submit(getData, dataset_id, code, min_date, limit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "proud-guide",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note that here measurement devices are named by their codes: names should be\n",
    "retrieved from the dataset _meteocat_xema_variables_ with its own dataset\n",
    "identifier. Below is given an example of how measurement codes could be\n",
    "renamed by its corresponded name-description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to get time-value table for each measurement device\n",
    "df_sub = df[[\"data_lectura\", \"codi_variable\", \"valor_lectura\"]]\n",
    "df_sub = df_sub.pivot(\"data_lectura\", \"codi_variable\")\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of code to variable names\n",
    "df_variables[\"nom_variable_unitat\"] = (\n",
    "    df_variables[\"nom_variable\"] + \" (\" + df_variables[\"unitat\"] + \")\"\n",
    ")\n",
    "dict_var = df_variables.set_index(\"codi_variable\")[\"nom_variable_unitat\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each weather station, save data in a CSV-file\n",
    "for code in codes:\n",
    "    df = pd.read_parquet(f\"{path}/dataset_{code}.parquet\")\n",
    "    # Select a station and pivot\n",
    "    df_station = df.query(f\"codi_estacio == '{code}'\")\n",
    "    df_station = df_station[~df_station[[\"data_lectura\", \"codi_variable\"]].duplicated()]\n",
    "    df_station = df_station.pivot(\"data_lectura\", \"codi_variable\", \"valor_lectura\")\n",
    "    # Change column type to string\n",
    "    df_station.columns = df_station.columns.map(str)\n",
    "    df_station = df_station.rename(columns=dict_var)\n",
    "    # Save table in a CSV-file\n",
    "    df_station.to_csv(f\"{path}/{code}_station.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-harbor",
   "metadata": {},
   "source": [
    "Finally, just plot the last station from the above loop to get a visual idea\n",
    "of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df_station.plot(figsize=(10, 10), title=f\"Weather station with code {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-leone",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
